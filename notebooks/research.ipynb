{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.DimondPricePrediction.logger import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.DimondPricePrediction.exception import customexception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.DimondPricePrediction.logger import logging\n",
    "from src.DimondPricePrediction.exception import customexception\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestionConfig:\n",
    "    raw_data_path:str=os.path.join(\"artifacts\",\"raw.csv\")\n",
    "    train_data_path:str=os.path.join(\"artifacts\",\"train.csv\")\n",
    "    test_data_path:str=os.path.join(\"artifacts\",\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    def __init__(self):\n",
    "        self.ingestion_config=DataIngestionConfig()\n",
    "        \n",
    "    \n",
    "    def initiate_data_ingestion(self):\n",
    "        logging.info(\"data ingestion started\")\n",
    "        \n",
    "        try:\n",
    "            data=pd.read_csv(Path(os.path.join(\"notebooks/data\",\"gemstone.csv\")))\n",
    "            logging.info(\" i have read dataset as a df\")\n",
    "            \n",
    "            \n",
    "            os.makedirs(os.path.dirname(os.path.join(self.ingestion_config.raw_data_path)),exist_ok=True)\n",
    "            data.to_csv(self.ingestion_config.raw_data_path,index=False)\n",
    "            logging.info(\" i have saved the raw dataset in artifact folder\")\n",
    "            \n",
    "            logging.info(\"here i have performed train test split\")\n",
    "            \n",
    "            train_data,test_data=train_test_split(data,test_size=0.25)\n",
    "            logging.info(\"train test split completed\")\n",
    "            \n",
    "            train_data.to_csv(self.ingestion_config.train_data_path,index=False)\n",
    "            test_data.to_csv(self.ingestion_config.test_data_path,index=False)\n",
    "            \n",
    "            logging.info(\"data ingestion part completed\")\n",
    "            \n",
    "            return (\n",
    "                 \n",
    "                \n",
    "                self.ingestion_config.train_data_path,\n",
    "                self.ingestion_config.test_data_path\n",
    "            )\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "           logging.info(\"exception during occured at data ingestion stage\")\n",
    "           raise customexception(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_transformatoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from src.DimondPricePrediction.exception import customexception\n",
    "from src.DimondPricePrediction.logger import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.DimondPricePrediction.utils.utils import save_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    preprocessor_obj_file_path=os.path.join('artifacts','preprocessor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self):\n",
    "        self.data_transformation_config=DataTransformationConfig()\n",
    "\n",
    "        \n",
    "    \n",
    "    def get_data_transformation(self):\n",
    "        \n",
    "        try:\n",
    "            logging.info('Data Transformation initiated')\n",
    "            \n",
    "            # Define which columns should be ordinal-encoded and which should be scaled\n",
    "            categorical_cols = ['cut', 'color','clarity']\n",
    "            numerical_cols = ['carat', 'depth','table', 'x', 'y', 'z']\n",
    "            \n",
    "            # Define the custom ranking for each ordinal variable\n",
    "            cut_categories = ['Fair', 'Good', 'Very Good','Premium','Ideal']\n",
    "            color_categories = ['D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "            clarity_categories = ['I1','SI2','SI1','VS2','VS1','VVS2','VVS1','IF']\n",
    "            \n",
    "            logging.info('Pipeline Initiated')\n",
    "            \n",
    "            ## Numerical Pipeline\n",
    "            num_pipeline=Pipeline(\n",
    "                steps=[\n",
    "                ('imputer',SimpleImputer(strategy='median')),\n",
    "                ('scaler',StandardScaler())\n",
    "\n",
    "                ]\n",
    "\n",
    "            )\n",
    "            \n",
    "            # Categorigal Pipeline\n",
    "            cat_pipeline=Pipeline(\n",
    "                steps=[\n",
    "                ('imputer',SimpleImputer(strategy='most_frequent')),\n",
    "                ('ordinalencoder',OrdinalEncoder(categories=[cut_categories,color_categories,clarity_categories])),\n",
    "                ('scaler',StandardScaler())\n",
    "                ]\n",
    "\n",
    "            )\n",
    "            \n",
    "            preprocessor=ColumnTransformer([\n",
    "            ('num_pipeline',num_pipeline,numerical_cols),\n",
    "            ('cat_pipeline',cat_pipeline,categorical_cols)\n",
    "            ])\n",
    "            \n",
    "            return preprocessor\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.info(\"Exception occured in the initiate_datatransformation\")\n",
    "\n",
    "            raise customexception(e,sys)\n",
    "            \n",
    "    \n",
    "    def initialize_data_transformation(self,train_path,test_path):\n",
    "        try:\n",
    "            train_df=pd.read_csv(train_path)\n",
    "            test_df=pd.read_csv(test_path)\n",
    "            \n",
    "            logging.info(\"read train and test data complete\")\n",
    "            logging.info(f'Train Dataframe Head : \\n{train_df.head().to_string()}')\n",
    "            logging.info(f'Test Dataframe Head : \\n{test_df.head().to_string()}')\n",
    "            \n",
    "            preprocessing_obj = self.get_data_transformation()\n",
    "            \n",
    "            target_column_name = 'price'\n",
    "            drop_columns = [target_column_name,'id']\n",
    "            \n",
    "            input_feature_train_df = train_df.drop(columns=drop_columns,axis=1)\n",
    "            target_feature_train_df=train_df[target_column_name]\n",
    "            \n",
    "            \n",
    "            input_feature_test_df=test_df.drop(columns=drop_columns,axis=1)\n",
    "            target_feature_test_df=test_df[target_column_name]\n",
    "            \n",
    "            input_feature_train_arr=preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "            \n",
    "            input_feature_test_arr=preprocessing_obj.transform(input_feature_test_df)\n",
    "            \n",
    "            logging.info(\"Applying preprocessing object on training and testing datasets.\")\n",
    "            \n",
    "            train_arr = np.c_[input_feature_train_arr, np.array(target_feature_train_df)]\n",
    "            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]\n",
    "\n",
    "            save_object(\n",
    "                file_path=self.data_transformation_config.preprocessor_obj_file_path,\n",
    "                obj=preprocessing_obj\n",
    "            )\n",
    "            \n",
    "            logging.info(\"preprocessing pickle file saved\")\n",
    "            \n",
    "            return (\n",
    "                train_arr,\n",
    "                test_arr\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.info(\"Exception occured in the initiate_datatransformation\")\n",
    "\n",
    "            raise customexception(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from src.DimondPricePrediction.logger import logging\n",
    "from src.DimondPricePrediction.exception import customexception\n",
    "from dataclasses import dataclass\n",
    "from src.DimondPricePrediction.utils.utils import save_object\n",
    "from src.DimondPricePrediction.utils.utils import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge,Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class ModelTrainerConfig:\n",
    "    trained_model_file_path = os.path.join('artifacts','model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class ModelTrainer:\n",
    "    def __init__(self):\n",
    "        self.model_trainer_config = ModelTrainerConfig()\n",
    "    \n",
    "    def initate_model_training(self,train_array,test_array):\n",
    "        try:\n",
    "            logging.info('Splitting Dependent and Independent variables from train and test data')\n",
    "            X_train, y_train, X_test, y_test = (\n",
    "                train_array[:,:-1],\n",
    "                train_array[:,-1],\n",
    "                test_array[:,:-1],\n",
    "                test_array[:,-1]\n",
    "            )\n",
    "\n",
    "            models={\n",
    "            'LinearRegression':LinearRegression(),\n",
    "            'Lasso':Lasso(),\n",
    "            'Ridge':Ridge()\n",
    "        }\n",
    "            \n",
    "            model_report:dict=evaluate_model(X_train,y_train,X_test,y_test,models)\n",
    "            print(model_report)\n",
    "            print('\\n====================================================================================\\n')\n",
    "            logging.info(f'Model Report : {model_report}')\n",
    "\n",
    "            # To get best model score from dictionary \n",
    "            best_model_score = max(sorted(model_report.values()))\n",
    "\n",
    "            best_model_name = list(model_report.keys())[\n",
    "                list(model_report.values()).index(best_model_score)\n",
    "            ]\n",
    "            \n",
    "            best_model = models[best_model_name]\n",
    "\n",
    "            print(f'Best Model Found , Model Name : {best_model_name} , R2 Score : {best_model_score}')\n",
    "            print('\\n====================================================================================\\n')\n",
    "            logging.info(f'Best Model Found , Model Name : {best_model_name} , R2 Score : {best_model_score}')\n",
    "\n",
    "            save_object(\n",
    "                 file_path=self.model_trainer_config.trained_model_file_path,\n",
    "                 obj=best_model\n",
    "            )\n",
    "          \n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info('Exception occured at Model Training')\n",
    "            raise customexception(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
